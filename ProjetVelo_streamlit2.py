# -*- coding: utf-8 -*-
#"""Test_Streamlit2.ipynb

#Automatically generated by Colab.

#Original file is located at
#    https://colab.research.google.com/drive/1ZeXSZMVq1dV4zY4k4xeFhLa01lTzdgRl
#"""

import streamlit as st
import time
import streamlit.components.v1 as components
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import numpy as np
import io
import base64
from PIL import Image

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, FunctionTransformer

from sklearn.decomposition import PCA
from sklearn.pipeline import FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import confusion_matrix

from sklearn.model_selection import cross_val_predict, cross_val_score, KFold, GridSearchCV
from sklearn.metrics import mean_squared_log_error, make_scorer, mean_squared_error, r2_score, mean_absolute_error
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import RandomizedSearchCV

from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.feature_selection import RFE

from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, RidgeCV

from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import ElasticNetCV, ElasticNet

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, IsolationForest

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns

import time
import joblib

# Désactivation des messages d'avertissements
import warnings
warnings.filterwarnings("ignore")

# travail sur carte
import geopandas as gpd #Gestion données spatiale
import json
import shapely #Manipulation de géométrie
from shapely.geometry import Point
import folium
from folium.plugins import MarkerCluster, MousePosition
from folium.features import GeoJson
from folium.features import GeoJsonTooltip
from folium.plugins import GroupedLayerControl
from streamlit_folium import folium_static
from branca.colormap import linear
from folium.plugins import GroupedLayerControl
from branca.colormap import linear
#______________________ ZONE GAUCHE DE DEFILEMENT DU SITE + MISE EN PAGE DROITE

# CSS  pour agrandir la taille du corps principal
st.markdown("""
    <style>
        .main .block-container {
            max-width: 1200px;
            padding-top: 2rem;
            padding-right: 2rem;
            padding-left: 2rem;
            padding-bottom: 1rem;
        }
        .main .element-container {
            max-width: 100%;
        }

    </style>
    """, unsafe_allow_html=True)

# Menu latéral
st.sidebar.title("Sommaire")
pages = ["Contexte et problématique projet", "Jeu de données","Exploration Statistique", "Visualisation des données", "Synthèse des tests de modélisation", "Prédictions", "Conclusion"]
page = st.sidebar.radio("Aller vers", pages)

def get_base64_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# Chemin de l'image
image_path = 'C:\\Users\\La Foune\\Documents\\Streamlit\\LinkedIn-Symbole.png'
linkedin_icon_base64 = get_base64_image(image_path)

# Affichage dans la sidebar du cadre perso
st.sidebar.markdown(
    f"""
    <div style='background-color: #333333; padding: 30px; margin-top: 50px;'>
        <p style='font-size: inherit; color: #ffffff; margin: 0;'>
            Formation continue Datascientest <br>
            Data Analyst Juillet 2024 <br>
            <a href="https://www.linkedin.com/in/alexandra-heurtevent-7a2532277/" 
               target="_blank" 
               style="color: #ffffff; text-decoration: underline; font-weight: bold;">
                Alexandra HEURTEVENT
            </a>
            <a href="https://www.linkedin.com/in/alexandra-heurtevent-7a2532277/" target="_blank">
            <img src="data:image/png;base64,{linkedin_icon_base64}" 
            alt="LinkedIn Icon" 
            style="width: 40px; height: 30px; margin-top: 0px; float: right;">
        </a>
        </p>

    </div>
    """,
    unsafe_allow_html=True
)

# Affichage des autres logos dans la barre latérale
st.sidebar.markdown(
    """
    <div style="display: flex; justify-content: space-between; margin-top: 50px;">
        <img src="https://www.je-change-de-metier.com/media/cache/customer_logo_webp/images/customers/datascientest-20logo-300x300-64e3c5b8a4319118220559.png" 
             alt="Logo Datascientest" 
             style="width: 50px;">
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Ville_de_Paris_logo.svg/592px-Ville_de_Paris_logo.svg.png" 
             alt="Logo Ville de Paris" 
             style="width: 50px;">
    </div>
    """,
    unsafe_allow_html=True
)


# Chemin de l'image
image_path = "C:\\Users\\La Foune\\Documents\\Streamlit\ImageVelo.png"
img = Image.open(image_path)
width = 1000   
height = 250  
img = img.resize((width, height))  
st.image(img, caption="Projet Vélo Paris")

st.title("Paris et son projet vélo")

if st.button('Recharger la page'):
    st.experimental_rerun()

#__________________ Page 1 CADRE PROJET
if page == pages[0] : 
    st.subheader("Contexte du projet")
    st.write("La ville de Paris a mis en place 2 plans vélo ambitieux pour encourager l'utilisation du vélo comme moyen de transport principal.")

#______________________Définition des plans vélo
    plan_2015_2020 = {
        "Nom": "Premier plan vélo 2015-2020",
        "Objectif": "Atteindre 15% des déplacements à vélo",
        "Points clés": [
            "Mise en place d'un réseau structurant REVe (Réseau express vélo) Nord-Sud et Est-Ouest, le long des berges de la Seine et avec connexion des bois de Vincennes et de Boulogne",
            "Développement d'un réseau secondaire pour résorber les coupures urbaines (ponts, places, portes)"
        ]
    }

    plan_2021_2026 = {
        "Nom": "Second plan vélo 2021-2026",
        "Objectif": "Faire de Paris une ville 100% cyclable",
        "Points clés": [
            "Augmentation des stationnements sécurisés",
            "Expansion du réseau cyclable sécurisé sur les grands axes",
            "Développement des double-sens cyclables dans les petites rues"
        ]
    }

    # Fonction pour formater les points clés avec couleur et style
    def format_points_cles(points_cles):
        formatted_text = ""
        for point in points_cles:
            formatted_text += f"<li style='font-weight: bold; color: #1f77b4;'>{point}</li>"
        return formatted_text


    # Création du tableau pour présenter les plans vélo avec mise en forme personnalisée
    table_data = [
        {"Plan vélo": f"<span style='color: #d62728;'>{plan_2015_2020['Nom']}</span>",
         "Objectif": f"<span style='font-weight: bold;'>{plan_2015_2020['Objectif']}</span>",
         "Points clés": format_points_cles(plan_2015_2020['Points clés'])},
        {"Plan vélo": f"<span style='color: #9467bd;'>{plan_2021_2026['Nom']}</span>",
         "Objectif": f"<span style='font-weight: bold;'>{plan_2021_2026['Objectif']}</span>",
         "Points clés": format_points_cles(plan_2021_2026['Points clés'])}
    ]

    # Affichage du tableau avec HTML dans une section dépliante
    with st.expander("Voir les détails des plans vélo"):
        table_html = "<table border='1' style='border-collapse: collapse; width: 100%;'>"
        table_html += "<tr>"
        table_html += "".join([f"<th style='padding: 10px; background-color: #f2f2f2;'>{col}</th>" for col in table_data[0].keys()])
        table_html += "</tr>"
        for row in table_data:
            table_html += "<tr>"
            for val in row.values():
                table_html += f"<td style='padding: 10px;'>{val}</td>"
            table_html += "</tr>"
        table_html += "</table>"
        st.markdown(table_html, unsafe_allow_html=True)

#______________________Sous titre 2 - Problématique du projet
    st.subheader("Problématique du projet")
    st.markdown(
        """
        Les compteurs enregistrent le nombre de vélo par heure, et fournissent une vue détaillée de l'utilisation du vélo tout au long de la journée, et selon les différentes zones géographiques.
        L'objectif de cette étude est de réaliser une analyse des données enregistrées par les compteurs à vélo afin de visualiser les tendances de circulation, les horaires, les principales zones d'affluence et d'identifier les facteurs influençant le trafic.


        """
    )
    with st.expander("Voir les problématiques spécifiques abordées"):
        st.markdown(
            """
            <style>
                .problematique {
                    margin-top: 25px;
                }
                .problem-title {
                    font-weight: bold;
                }
                .problem-text {
                    color: #1f77b4;
                    font-weight: bold;
                    margin-top: 10px;
                }
                .question-text {
                    font-style: italic;
                }
            </style>

            <div class='problematique'>
                <span class='problem-title'>1. Compréhension des tendances de circulation :</span><br>
                <span class='question-text'>Est-ce que les facteurs temporels peuvent impacter la fréquentation des vélos ?</span><br>
                <div class='problem-text'>
                    Analyser les tendances par heure, par tranches horaires, les tendances quotidiennes, hebdomadaires et saisonnières.
                </div>
            </div>

            <div class='problematique'>
                <span class='problem-title'>2. Évaluation de la répartition des infrastructures cyclables :</span><br>
                <span class='question-text'>La qualité des infrastructures cyclables (piste cyclable, voie bus…) impacte-t-elle la fréquentation des zones par les cyclistes ?</span><br>
                <div class='problem-text'>
                    Analyser sur une carte la répartition de la fréquentation et des infrastructures.
                </div>
            </div>

            <div class='problematique'>
                <span class='problem-title'>3. Analyse géographique :</span><br>
                <span class='question-text'>Est-ce que les infrastructures cyclables et la géographie, notamment le découpage par arrondissement, influent sur la fréquentation des vélos ?</span><br>
                <div class='problem-text'>
                    Analyser si la fréquentation de différents compteurs.
                </div>
            </div>

            <div class='problematique'>
                <span class='problem-title'>4. Influence des facteurs externes :</span><br>
                <span class='question-text'>Dans quelle mesure les facteurs externes tels que la météo, les événements spéciaux comme les vacances et les jours fériés, ou les grèves, influencent-ils l’activité cycliste à Paris ?</span><br>
                <div class='problem-text'>
                    Étudier par des graphiques les différents facteurs.
                </div>
            </div>
            """,
            unsafe_allow_html=True
        )

#__________________ JEU DE DONNEES
elif page == pages[1]:
    st.subheader("Jeu de données")
    st.write("""
    La variable cible est **Comptage_horaire**, la seule variable numérique correspondant au nombre de vélos/heure/par compteur.
    Les variables explicatives par défaut dans le jeu de données sont de dimensions temporelles et géographiques.      
    
    **L'analyse exploratoire concerne la période du 01 janvier 2021 au 31 janvier 2024.**
             
    **L'étude de prévision (machine learning) concerne la période du 01 avril 2021 au 31 mars 2022**.         
             """)
   
    with st.expander("Voir les différentes actions réalisées sur le dataframe"):
        # Fonction pour charger les données
        @st.cache_data
        def load_data(filepath):
            return pd.read_csv(filepath)

        csv_file_path = 'C:\\Users\\La Foune\\Documents\\Streamlit\\df_pre_modelisation.csv'
        df = load_data(csv_file_path)

        # Utilisation de StringIO pour capturer les informations du DataFrame en tant que texte
        buffer = io.StringIO()
        df.info(buf=buffer)
        info_str = buffer.getvalue()


        st.markdown("<span style='font-size: 20px; font-weight: bold;'>Dataset FINAL</span>", unsafe_allow_html=True)
        df_head = df.head(2)
        st.dataframe(df_head, use_container_width=True)

        col1, col2 = st.columns([1, 1])
        with col1:
            st.markdown("<span style='font-size: 20px; font-weight: bold;'>Dataset initial</span>", unsafe_allow_html=True)
            
            # Définir les données pour le tableau
            data_description = [
                {"Nom": "Nombre d'entrées / lignes", "Valeurs": "2 066 595"},
                {"Nom": "Nombre de colonnes", "Valeurs": "16"},
                {"Nom": "Valeurs manquantes totales", "Valeurs": "311 010"},
                {"Nom": "Nombre de doublons", "Valeurs": "0"},
                {"Nom": "Variables numériques", "Valeurs": "2"},
                {"Nom": "Variables catégorielles", "Valeurs": "14"},
                {"Nom": "Autres types de variable", "Valeurs": "0"},
                {"Nom": "Colonnes avec des valeurs manquantes", "Valeurs": "6"},
                {"Nom": "Pourcentage de valeurs manquantes", "Valeurs": "0.01%"},
                {"Nom": "Date de début", "Valeurs": "2012-02-13"},
                {"Nom": "Date de fin", "Valeurs": "2024-02-08"}
            ]

            def format_description(description):
                return f"<span style='font-weight: normal; color: #1f77b4;'>{description}</span>"

            table_data = [
                {"Info dataset initial": f"<span style='color: #404040;'>{item['Nom']}</span>",
                "Valeurs": format_description(item['Valeurs'])}
                for item in data_description
            ]
            # Affichage avec HTML
            table_html = "<table border='1' style='border-collapse: collapse; width: 70%;'>"
            table_html += "<tr>"
            table_html += "".join([f"<th style='padding: 4px 8px; background-color: #f2f2f2;'>{col}</th>" for col in table_data[0].keys()])
            table_html += "</tr>"
            for row in table_data:
                table_html += "<tr>"
                for val in row.values():
                    table_html += f"<td style='padding: 4px 8px;'>{val}</td>"
                table_html += "</tr>"
            table_html += "</table>"
            st.markdown(table_html, unsafe_allow_html=True)

            st.markdown("""
                - **Nombre de colonnes du dataset initial gardées : 5**
                    - **Id_Compteur**: Identifiant du compteur
                    - **Comptage_horaire**: Comptage horaire par compteur
                    - **Date_heure_comptage**: Date et heure du comptage
                    - **Coordonnees_geographiques**: Latitude et longitude
                
                **&#10132; Ajout de variables temporelles basées sur la date : semaine/week-end, tranche horaire, saison, numéro de semaine, jour, mois, ...**""")

        with col2:
            st.markdown("<span style='font-size: 20px; font-weight: bold;'>Ajout des variables complémentaires</span>", unsafe_allow_html=True)

            st.markdown("""
            **&#10132; Ajout de variables complémentaires : journées sans voiture, météo, journée de grève, jours fériés, vacances scolaires par zone en France**
                                                     
            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <!-- Le lien est retiré et remplacé par un style de texte -->
                    <p style="margin: 2px 0; line-height: 1.2; color: darkblue; font-weight: bold;">
                        Journée sans voiture
                    </p>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> Journee_sans_voiture
                    </p>
                </div>
            </div>                
                            
            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <span>
                        <a href="https://www.historique-meteo.net/france/ile-de-france/paris/2024/01/" target="_blank" style="color: darkblue; text-decoration: underline;font-weight: bold;">
                            Météo
                        </a> : (fichier .csv et .txt)
                    </span>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> Pluie, UV_soleil, Avis_meteo, Température, Météo
                    </p>
                </div>
            </div>

            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <span>
                        <a href="https://data.smartidf.services/explore/dataset/mouvements-sociaux-depuis-2002/information/" target="_blank" style="color: darkblue; text-decoration: underline;font-weight: bold;">
                            Grève
                        </a> : (fichier .csv)
                    </span>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> Date_début_greve, Date_fin_greve, motifs_greve
                    </p>
                </div>
            </div>           

            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <span>
                        <a href="https://www.data.gouv.fr/fr/datasets/vacances-scolaires-par-zones/" target="_blank" style="color: darkblue; text-decoration: underline;font-weight: bold;">
                            Vacances scolaires métropole par zone 
                        </a> : (fichier .csv)
                    </span>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> vacances_zone_a, vacances_zone_b, vacances_zone_c, Holidays
                    </p>
                </div>
            </div>
            <div style="height: 30px;"></div>             
                              
           **&#10132;  Ajout complémenataires lors de l'étude de la cartographie, données utilisées lors de la modélisation : arrondissement, typologie du réseau cyclable.**
            
            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <span>
                        <a href="https://opendata.paris.fr/explore/dataset/arrondissements/information/" target="_blank" style="color: darkblue; text-decoration: underline;font-weight: bold;">
                            Arrondissement 
                        </a> : (fichier .geojson)
                    </span>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> district, geometry (POINT)
                    </p>
                </div>
            </div>    

            <div style="line-height: 1.4;">
                <div style="margin-bottom: 20px;">
                    <span>
                        <a href="https://opendata.paris.fr/explore/dataset/reseau-cyclable/information/" target="_blank" style="color: darkblue; text-decoration: underline;font-weight: bold;">
                            Typologie et aménagement cyclable 
                        </a> : (fichier .geojson)
                    </span>
                    <p style="margin: 2px 0; line-height: 1.2; color: black;">
                        <strong>Colonne(s) :</strong> typologie_simple, amenagement_bidirectionnel, regime_vitesse, sens_velo, bois, couloir_bus, reseau_REVe, geometry (LINESTRING)
                    </p>
                </div>
            </div>    

            """, unsafe_allow_html=True)

    #________________________________________
    st.subheader("Exploration des données")   
    with st.expander("Voir les différentes actions réalisées sur le dataframe"):
        col1, col2 = st.columns([1, 1])
        with col2:
            st.markdown("<span style='font-size: 20px; font-weight: bold;'>Info dataset final</span>", unsafe_allow_html=True)
            st.text(info_str)

        with col1:
            st.markdown("<span style='font-size: 20px; font-weight: bold;'>Actions</span>", unsafe_allow_html=True)
            
            st.markdown("""

            - **Suppression des valeurs manquantes :**
                - Absence de certains liens hypertextes pour afficher les photos      
                - Absence de la météo et de la température entre 23h et 5h du matin (soit 6.2% du nbre de vélos)
            """, unsafe_allow_html=True)
            
            csv_file_path = 'C:\\Users\\La Foune\\Documents\\Streamlit\\comptage_final_suppNan.csv'
            comptage_final = load_data(csv_file_path)
            # Créer un graphique
            fig = px.bar(comptage_final, x='Température', y='Comptage_final', color='Température',
                        labels={'Température': 'Valeurs nulles ou non-nulles de Température', 'Comptage_final': 'Comptage du nombre de vélos'},
                        title='Part des valeurs manquantes ou non (en prenant la variable Température)')
            for i, row in comptage_final.iterrows():
                fig.add_annotation(x=row['Température'], y=row['Comptage_final'],
                                text=f"{row['Pourcentage']:.1f}%", showarrow=False,
                                font=dict(color='black', size=12))
            st.plotly_chart(fig)


    st.subheader("Langage PYTHON et librairies utilisées") 
    with st.expander("Voir les librairies utilisées"):
        # Définir les données pour le tableau
        langages_et_librairies = [
            {"Nom": "pandas",
            "Description": "Manipulation et analyse de données"},
            {"Nom": "numpy",
            "Description": "Calcul numérique" },
            {"Nom": "statsmodels",
            "Description": "Modélisation statistique"},
            {"Nom": "scikit-learn",
            "Description": "Apprentissage automatique et modélisation" },
            {"Nom": "matplotlib",
            "Description": "Visualisation de données statiques" },
            {"Nom": "plotly",
            "Description": "Visualisation de données interactives"},
            {"Nom": "shapely",
            "Description": "Manipulation de géométries"},
            {"Nom": "folium",
            "Description": "Cartographie interactive"},
            {"Nom": "geopandas",
            "Description": "Analyse géospatiale"},
            {"Nom": "json",
            "Description": "Manipulation de données JSON"},
            {"Nom": "html",
            "Description": "Affichage contenu web utilisé pour Streamlit"},        
           {"Nom": "io",
            "Description": "Gestion des flux de données en mémoire"},    
            {"Nom": "joblib",
            "Description": "Sauvegarde / chargement de modèle"}    
        ]

        # Fonction couleur et style
        def format_description(description):
            return f"<span style='font-weight: normal; color: #1f77b4;'>{description}</span>"

        # Création du tableau 
        table_data = [
            {"Langage PYTHON avec librairie": f"<span style='color: #d62728;'>{item['Nom']}</span>",
            "Description": format_description(item['Description'])}
            for item in langages_et_librairies
        ]

        # Affichage avec HTML
        table_html = "<table border='1' style='border-collapse: collapse; width: 50%;'>"
        table_html += "<tr>"
        table_html += "".join([f"<th style='padding: 4px 8px; background-color: #f2f2f2;'>{col}</th>" for col in table_data[0].keys()])
        table_html += "</tr>"
        for row in table_data:
            table_html += "<tr>"
            for val in row.values():
                table_html += f"<td style='padding: 4px 8px;'>{val}</td>"
            table_html += "</tr>"
        table_html += "</table>"
        st.markdown(table_html, unsafe_allow_html=True)



    #______________________Section "Sources de données" dans le corps principal
    st.subheader("Sources de données")
    with st.expander("Accéder aux sources utilisées", expanded=False):
        st.markdown("""
            <div style="line-height: 1.4;">
                                       
            <div style="margin-bottom: 20px;">
                <a href="https://www.historique-meteo.net/france/ile-de-france/paris/2024/01/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Historique Météo</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : les données météorologiques fournies par la société WorldWeatherOnline</p>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Documentation pour les Weather codes</strong> : <a href="https://www.historique-meteo.net/weathercodes.txt" target="_blank">Weather codes</a></p>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Données exportées</strong> : Paris LAT/LON: 48.856614/2.3522219. Une table de correspondance interprète les codes météos et généralise les états de la météo.</p>
            </div>

            <div style="margin-bottom: 20px;">
                <a href="https://data.smartidf.services/explore/dataset/mouvements-sociaux-depuis-2002/information/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Mouvements sociaux depuis 2002</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : Open Database License (ODbL)</p>
            </div>

            <div style="margin-bottom: 20px;">
                <a href="https://www.data.gouv.fr/fr/datasets/vacances-scolaires-par-zones/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Vacances Scolaires</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : Licence Ouverte / Open Licence</p>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Données</strong> : Vacances identifiées par zone A, B, C pour toute la France.</p>
            </div>

            <div style="margin-bottom: 20px;">
                <a href="https://www.data.gouv.fr/fr/datasets/jours-feries-en-france/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Jours Fériés</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : Licence Ouverte / Open Licence version 2.0</p>
            </div>

            <div style="margin-bottom: 20px;">
                <a href="https://opendata.paris.fr/explore/dataset/reseau-cyclable/information/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Réseau Cyclable - fichier .geojson</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : Open Database License (ODbL)</p>
            </div>

            <div style="margin-bottom: 20px;">
                <a href="https://opendata.paris.fr/explore/dataset/arrondissements/information/" target="_blank">
                    <button style="background-color: #1f77b4; color: white; border: none; padding: 2px 12px; text-align: center; text-decoration: none; display: inline-block; font-size: 14px; margin: 2px; cursor: pointer;">Arrondissements - fichier .geojson</button>
                </a>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Licence</strong> : Open Database px (ODbL)</p>
                <p style="margin: 2px 0; line-height: 1.2;"> <strong>Description</strong> : Délimitation des arrondissements par le décret impérial du 1er novembre 1859. Un arrondissement est formé d’un seul polygone en respectant une topologie de surfaces.</p>
            </div>

            </div>
        """, unsafe_allow_html=True)

#___________________Page 3 EXPLORATION 
elif page == pages[2]:  
    st.subheader("Statistique exploratoire")
    st.write("L'analyse de la distribution des valeurs vise à fournir une compréhension approfondie de la structure et de la nature des données, essentielle pour orienter les analyses ultérieures et préparer le travail de modélisation pour faciliter la sélectoin du modèle approprié pour les prédictions. Comment sont réparties ou regroupées les valeurs ?")
    st.write("L'analyse des indicateurs de position et de dispersion inclue la moyenne, l'écart type, les quartiles, la médiane, les valeurs minimales et maximales, les valeurs d'asymétrie et d'applatissement afin d'inspecteur la normalité de la distribution.")

    with st.expander("Voir les statistiques exploratoires"):
        col1, col2 = st.columns([1, 1])
        with col1:
            with open("C:\\Users\\La Foune\\Documents\\Streamlit\\Graph_histogramme.html", 'r', encoding='utf-8') as file:
                html_content6 = file.read()
            components.html(
                f"""
                <div style="position: relative; left: -20px;">
                    {html_content6}
                </div>
                """,
                height=400,
                width=600,
            )

            # Création du DataFrame avec les données
            data = {
                'Mesure': [
                    'Coefficient d\'asymétrie Skewness',
                    'Valeur d\'applatissement Kurtosis',
                    'Statistique du test K2 d\'Agostino Pearson',
                    'Valeur P K2 de D\'Agostino Pearson',
                ],
                'Valeur': [
                    '2.6160175',
                    '10.01693829',
                    '580963.23830573',
                    '0.0',
                ]}
            df = pd.DataFrame(data)
            st.dataframe(df, use_container_width=True, hide_index=True)
            st.markdown(
                """
                <h1 style="font-size:200px;"> </h1>
                """,
                unsafe_allow_html=True
            )
        with col2:
            st.markdown(
                """                 
                ### Points Clés :
                - **Nbre de lignes : 1 025 102**
                - **Médiane (quartile 50%) : 37**<br>
                    &#10132; Distribution des données avec une asymétrie positive, marquée par une longue traîne de valeurs ce qui élève la moyenne à **environ 66.13**.<br>
                    &#10132; Variabilité avec des nombres moins importants de passages, mais ponctuée par des pics significatifs jusqu'à **1302 passages**.<br>
                    &#10132; Distortion de la moyenne, répartition inégale.<br>

                - **Écart-type : 86,10**<br>
                    &#10132; La différence importante entre l'écart-type (86,10) et la moyenne (66,13) suggère que les données ne sont pas concentrées autour de la moyenne mais plutôt éparpillées sur une large plage de valeurs.<br>

                - **Quartile 75% (Q3) : 75% des valeurs sont inférieures ou égales à 89.**

                - **Coefficient d'asymétrie (Skewness) : 2,62**<br>
                    *Mesure l'asymétrie de la distribution des donnée par rapport à une distribution normale symétrique.<br>*
                    &#10132; Coefficient d'asymétrie positif : distribution asymétrique vers la droite.

                - **Valeur d'applatissement (Kurtosis) : 10,01**<br> 
                    *Mesure la "queue" et la forme d'une distribution par rapport à une distribution normale.<br>*
                    &#10132; Si l'applatissement est supérieur à 3 : présence de fortes valeurs extrêmes ou aberrantes et une "queue" longue et fine.

                - **Test de D'Agostino-Pearson (K²) : Statistique du test = 580963.23830573 - Valeur P K2 = nulle**<br>
                    *Evaluer la normalité d'une distribution de données.<br>* 
                    &#10132; Rejet de l'hypothèse nulle selon laquelle les données suivent une distribution normale.  

                **Ces visualisations aident à mieux comprendre la structure et les caractéristiques des données avant de procéder à une analyse plus approfondie.**

                """,
                unsafe_allow_html=True
            )

    st.subheader("Matrice de corrélation")
    with st.expander("Voir la matrice de corrélation"):
        col1, col2 = st.columns([1, 1])
        with col1:
            with open("C:\\Users\\La Foune\\Documents\\Streamlit\\Graph_correlation.html", 'r', encoding='utf-8') as file:
                    html_content8 = file.read()
                    components.html(
                            f"""
                            <div style="width: 100%; height: 400px; overflow: hidden;margin-bottom: 80px;">
                                {html_content8}
                            </div>
                            """,
                            height=400,
                            width=600,
                        )
            st.image("C:\\Users\\La Foune\\Documents\\Streamlit\\Heatmap_2.jpg", caption='Heatmap 2', use_column_width=True)


        with col2:
            st.markdown(
                """
                *Plus le coefficient de corrélation s'éloigne de zéro (positivement ou négativement), plus la relation entre les variables est forte.*
                - **Corrélation positive**
                **Heure** : corrélation positive modérée avec Comptage_horaire (0.29) &#10132; l'heure est fortement associée à des variations. Les heures de pointe peuvent influencer positivement le nombre de passages.
                - **Corrélation négative**
                **Num_jour** : corrélation négative faible avec Comptage_horaire (-0.10). Relation inverse entre jour de la semaine et comptage horaire avec une possible influence entre semaine et week-end ou/et entre les jours.
                """,
                unsafe_allow_html=True
            )
        
       
    st.subheader("Test Anova")
    with st.expander("Voir le test Anova"):
        st.write("Les test ANOVA est utilisé pour évaluer la relation entre les variables indépendantes catégorielles et la variable dépendante continue (notre variable cible) et détecter les facteurs explicatifs qui influencent les résultats. ")
        st.write("**Les variables numériques sont traitées comme discrètes pour l'analyse ANOVA, les plus influentes étant celles avec les plus hautes valeurs de la statistique F comme Heure et Tranche_Horaire, Week_or_not ou Num_jour, Température. Ces variables ont un impact significatif sur le Comptage_horaire des vélos, tandis que d'autres ont des effets plus modestes.**")
        st.write("**Statistique F** : mesure la variance entre les groupes par rapport à la variance au sein des groupes. Si la valeur est élevée, les différents groupes ont des effect significatifs différents.")
        st.write("**P-Value** :  Indique que les différences entre les groupes sont statistiquement significatives ou non (seuil : 0,05).")

        with open("C:\\Users\\La Foune\\Documents\\Streamlit\\GraphAnnova.html", 'r', encoding='utf-8') as file:
            html_content6 = file.read()
        components.html(
            f"""
            <div style="position: relative; left: -20px;">
                {html_content6}
            </div>
            """,
            height=400,
            width=1100,  )


        col1, col2 = st.columns([2, 1])
        with col1: 
            data = {
                    'Hypothèse': ['H0', 'H1'],
                    'Description': [
                        'Il n\'y a pas d\'effet significatif de la variable catégorielle (qualitative) sur la variable continue (quantitative).',
                        'Il y a un effet significatif de la variable catégorielle (qualitative) sur la variable\n continue (quantitative).'
                        ]}
            df_hypotheses = pd.DataFrame(data)
            html_table = df_hypotheses.to_html(escape=False, index=False)
            st.markdown(html_table, unsafe_allow_html=True) 


        with col2:
            AnovaValue = {
                'Valeur': ['P-value ≤ 0,05', 'P-value > 0,05'],
                'Décision': [
                    "H0 est rejeté et on accepte H1",
                    "H0 n'est pas rejeté"
                    ]}                
            df_anova = pd.DataFrame(AnovaValue)
            st.dataframe(df_anova, use_container_width=False,hide_index=True) 

#___________________page DataVisualisation
elif page == pages[3]:  
    st.subheader("Visualisation des données")

    #___________Graphique ANNUEL ET EVENEMENTS SPECIAUX
    st.markdown(
        """
        <h2 style="text-align: left; font-size: 26px;">Analyse annuelle</h2>
        """,
        unsafe_allow_html=True
    )

    st.markdown(
        """
        <p style="text-align: left;">Peut-on dire que la pratique du vélo sur Paris est cyclique ? Est-elle influencée par la période, c'est-à-dire la semaine ou les week-ends, certains mois de l'année, les vacances scolaires ou les jours fériés ?</p>
        """,
        unsafe_allow_html=True
    )
    with st.expander("Voir les graphiques"):
        # Charger les données
        data_comptage_annee = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_comptage_annee.csv')
        data_evolution = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_evolution.csv')
        with open("C:\\Users\\La Foune\\Documents\\Streamlit\\ComptageEvenements.html", 'r', encoding='utf-8') as file:
            html_content3 = file.read()


        # Organiser les graphiques
        col1, col2 = st.columns(2)
        with col1:
            # Graphique par année
            fig_annee = px.line(data_comptage_annee, x="Mois", y="Comptage_horaire", color='Annee', title="Comptage horaire moyen par année")
            fig_annee['data'][0]['line']['color'] = 'deeppink'
            fig_annee['data'][1]['line']['color'] = 'blue'
            fig_annee['data'][2]['line']['color'] = 'darkBlue'
            fig_annee.add_hline(y=data_comptage_annee['Comptage_horaire'].mean(), line_dash="dot", annotation_text="Moyenne", annotation_position="top right")

            # Ajouter l'écart type et la moyenne
            mean_value = data_comptage_annee['Comptage_horaire'].mean()
            std_dev_value = data_comptage_annee['Comptage_horaire'].std()

            #Forme
            fig_annee.add_shape(
                type="rect",
                x0=0,
                y0=mean_value - std_dev_value,
                x1=13,
                y1=mean_value + std_dev_value,
                line=dict(color="lightsalmon", width=2),
                fillcolor="lightsalmon",
                opacity=0.2,
                name="Standard Deviation"
            )

            fig_annee.update_traces(mode="markers+lines", hovertemplate=None, textposition="top center")
            fig_annee.update_layout(
                hovermode="x unified",
                plot_bgcolor='aliceblue',
                legend=dict(
                    orientation="h",        
                    yanchor="bottom",         
                    y=-0.3,                   
                    xanchor="center",         
                    x=0.5,                    
                    title=None                                ))

            st.plotly_chart(fig_annee)

        # Graphique EVOLUTION
        with col2:
            colors = ['blue' if x >= 0 else 'darkBlue' for x in data_evolution['Evolution']]
            fig_evolution = px.bar(data_evolution, x="annee_mois", y="Evolution", text = round(data_evolution["Evolution"],0),color=colors, color_discrete_map={'blue': 'blue', 'darkBlue': 'darkBlue'}, title="Comptage horaire moyen 2022-2023 comparaison à n-1")

            # Obtention des valeurs min et max de l'axe y
            y_min = data_evolution["Evolution"].min()
            y_max = data_evolution["Evolution"].max()

            # Ajouter une ligne verticale à l'abscisse 1-2023
            fig_evolution.add_shape(type="line", x0="1-2023", y0=y_min, x1="1-2023", y1=y_max, line=dict(color="black", width=1, dash="dash"))

            # Ajouter une annotation pour le nom de la ligne verticale
            fig_evolution.add_annotation(x="1-2023", y=y_max,  
                                        text="Année n+1",  
                                        showarrow=False,  
                                        xshift=10,# Décalage horizontal
                                        yshift=20, # Décalage vertical
                                        font=dict(color="black")) 

            # Cadre 2022
            fig_evolution.add_annotation(
                x="8-2022",               
                y=-20,                    
                text="2022",                
                showarrow=False,            
                font=dict(
                    size=20,               
                    color="white"          
                ),
                align="center",            
                bgcolor="blue",             
                xanchor="center",           
                yanchor="middle"            
            )

            # Cadre 2023
            fig_evolution.add_annotation(
                x="7-2023",               
                y=20,                    
                text="2023",                
                showarrow=False,           
                font=dict(
                    size=20,              
                    color="white"          
                ),
                align="center",            
                bgcolor="Darkblue",             
                xanchor="center",        
                yanchor="middle"         
            )

            # Cacher la légende
            fig_evolution.update_traces( showlegend=False)

            # Mise à jour de la disposition pour un mode de survol
            fig_evolution.update_layout(hovermode="x unified")

            st.plotly_chart(fig_evolution)


        # Graphique événements spéciaux _ export graph html
        col3 = st.columns(1)[0]
        with col3:
            components.html(f"<div class='no-margin'>{html_content3}</div>", height=600, width = 1200, scrolling=True)
#______________________Graphique SAISON ET METEO
    st.markdown(
        """
        <h2 style="text-align: left; font-size: 26px;">Analyse Saison et Météo</h2>
        """,
        unsafe_allow_html=True
    )

    st.markdown(
        """
        <p style="text-align: left;">Peut-on dire que la pratique du vélo sur Paris est saisonnière ?</p>
        """,
        unsafe_allow_html=True
    )
    with st.expander("Voir les graphiques"):

        # Charger les données
        data_agg = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_agg.csv')
        data_pie = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_pie.csv')

        # Trouver l'index de la plus faible valeur
        min_index = data_pie['Comptage_horaire'].idxmin()

        # Créer une figure avec deux colonnes
        fig_combined = make_subplots(
            rows=1, cols=2,
            column_widths=[0.4, 0.6],
            subplot_titles=("Impact des saisons sur la fréquentation vélo", "Influence des semaines et week-ends sur les saisons"),
            specs=[[{"type": "domain"}, {"type": "xy"}]]
        )

        # Définition des couleurs pour le pie chart
        colors_pie = ["#2480fd", "#ed3947", "#153076", "#1ba2fe"]

        # Création du pie chart avec les données agrégées
        fig_pie = go.Figure(data=[go.Pie(
            labels=data_pie['Saison'],
            values=data_pie['Comptage_horaire'],
            textinfo='percent+label',
            marker=dict(colors=colors_pie),
            pull=[0.2 if i == min_index else 0 for i in range(len(data_pie))],
            showlegend=False
        )])

        # Ajouter le pie chart à la première colonne de la figure combinée
        for trace in fig_pie.data:
            fig_combined.add_trace(trace, row=1, col=1)

         #_______________________________Graphique à barre groupé
        fig_bar = px.bar(
            data_agg,
            x='Saison',
            y='Comptage_horaire',
            color='Week_or_not',
            barmode='group',
            labels={'Saison': 'Saison', 'Comptage_horaire': 'Comptage horaire'},
            color_discrete_map={'Semaine': 'lightblue', 'Weekend': 'blue'}
        )

        # Ajouter le graphique à barres groupées à la deuxième colonne de la figure combinée
        for trace in fig_bar.data:
            fig_combined.add_trace(trace, row=1, col=2)

        # Mise en forme des sous-graphiques
        fig_combined.update_layout(
            title_text="Analyse par saison en distinguant semaine / week-end",
            title_x=0,
            title_font_size=20,
            height=500,  
            width=1200,  
        )

        # Ajustement des axes pour le graphique à barres
        fig_combined.update_xaxes(title_text='Saison', row=1, col=2)
        fig_combined.update_yaxes(title_text='Comptage horaire', row=1, col=2)

        # Ajuster les axes du pie chart
        fig_combined.update_xaxes(title_text=None, row=1, col=1)
        fig_combined.update_yaxes(title_text=None, row=1, col=1)

        # Afficher la figure combinée dans Streamlit
        st.plotly_chart(fig_combined)

                ###________________________________ METEO


        # Graphiques valeurs météo
        # Charger les données
        data_avis_meteo = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_avis_meteo.csv')  
        data_meteo = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_meteo.csv')      
        data_uv = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_uv.csv')  
        data_temperature = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_temperature.csv')  

        # Création de la figure avec les sous-tracés
        fig_meteo = make_subplots(
            rows=2, cols=2,
            subplot_titles=("UV Soleil", "Température", "Avis météo", "Météo"),
            specs=[[{"type": "xy"}, {"type": "xy"}],
                [{"type": "xy"}, {"type": "xy"}]],
        vertical_spacing=0.2,  # Espacement vertical entre les lignes
        horizontal_spacing=0.10 # Espacement horizontal entre les colonnes
        )

        # Ajouter les traces pour chaque subplot
        fig_meteo.add_trace(go.Bar(x=data_avis_meteo["Avis_meteo"], y=data_avis_meteo["Comptage_horaire"], marker_color="Blue"), row=2, col=1)
        fig_meteo.add_trace(go.Bar(x=data_meteo["Météo"], y=data_meteo["Comptage_horaire"], marker_color="Blue"), row=2, col=2)
        fig_meteo.add_trace(go.Bar(x=data_uv["UV_soleil"], y=data_uv["Comptage_horaire"], marker_color="Blue"), row=1, col=1)
        fig_meteo.add_trace(go.Bar(x=data_temperature["Température"], y=data_temperature["Comptage_horaire"], marker_color="Blue"), row=1, col=2)

        # Mise à jour du layout de la figure
        fig_meteo.update_layout(
            title_text="Analyse de l'influence de la météo sur les comptages vélos",
            title_x=0,  
            title_font_size=20,  
            showlegend=False,
            height=500,  
            width=1400
        )

        # Mise à jour des titres des axes pour chaque subplot
        fig_meteo.update_yaxes(title_text="Comptage Horaire", row=1, col=1)
        fig_meteo.update_yaxes(title_text="Comptage Horaire", row=2, col=1)

        # Afficher la figure combinée dans Streamlit
        st.plotly_chart(fig_meteo)
#______________________Graphique TRANCHE HORAIRE 
    st.markdown(
        """
        <h2 style="text-align: left; font-size: 26px;">Analyse Tranche Horaire et Heures</h2>
        """,
        unsafe_allow_html=True
    )    

    st.markdown(
    """
    <p style="text-align: left;">Explication sur le graphique de la fréquentation par heure et tranche horaire.</p>
    """,
    unsafe_allow_html=True
)
    with st.expander("Voir les graphiques"):
        # Graphique TRANCHE HORAIRE ET SEMAINE OU Week-end 
        # Charger les données
        data_TrancheH_Week_or_not = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\TrancheH_Week_or_not.csv')  

        # Créer le graphique avec facettes
        fig_heure = px.bar(
            data_TrancheH_Week_or_not, 
            x='Heure', 
            y='Comptage_horaire', 
            text=round(data_TrancheH_Week_or_not["Comptage_horaire"], 0), 
            color='Tranche_horaire', 
            facet_col='Week_or_not',
            facet_col_spacing=0.1,  # Espacement entre les facettes
            color_discrete_map={
                '1-Créneau 6h - 10h': "Darkblue", 
                '2-Créneau 11h - 15h': "LightBlue", 
                '3-Créneau 16h - 20h': 'Blue', 
                '4-Créneau 21h - 23h': 'Pink', 
                '5-Créneau minuit - 5h (nuit)': 'DeepPink'
            }
        )

        # Mise à jour des annotations
        fig_heure.for_each_annotation(lambda a: a.update(text=a.text.split("=")[1]))

        # Mise à jour du layout
        fig_heure.update_layout(
            title_text="Moyenne des passages par tranche horaire en semaine / week-end",
            legend=dict(
                bgcolor='aliceblue', 
                bordercolor='black',
                orientation='h',  
                y=-0.2,  # Positionner la légende en bas
                x=0.5,
                xanchor='center'
            ),
            height=400,
            title_x=0,
            title_font_size=20
        )

        # Mettre à jour les traces pour positionner les textes en dehors des barres
        fig_heure.update_traces(
            textposition='outside',
            selector=dict(type='bar')
        )

        # Afficher le graphique dans Streamlit
        st.plotly_chart(fig_heure)

        #____________________ JOURS FERIES
        # Data Jours fériés
        jour_ferie = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\jour_ferie.csv')  

        # Graphique JOURS FERIES
        fig_jour_ferie = px.bar(
            jour_ferie,
            x='Heure',
            y='Comptage_horaire',
            text=round(jour_ferie["Comptage_horaire"], 0),
            color='Tranche_horaire',
            color_discrete_map={
                '1-Créneau 6h - 10h': "Darkblue",
                '2-Créneau 11h - 15h': "LightBlue",
                '3-Créneau 16h - 20h': 'Blue',
                '4-Créneau 21h - 23h': 'Pink',
                '5-Créneau minuit - 5h (nuit)': 'DeepPink'
            }
        )

        # Mettre à jour la disposition du graphique
        fig_jour_ferie.update_layout(
            showlegend= True,  # Retirer la légende
            legend=dict(
                orientation="h",  # Orientation horizontale
                yanchor="top",
                y=-0.2,  # Positionner la légende en bas
                xanchor="center",
                x=0.5
            ),
            xaxis_title='Heure',
            yaxis_title='Comptage Horaire',
            title_text='Moyenne des passages - jours fériés',
            title_x=0,
            title_font_size=20,
            xaxis_tickangle=-45,  # Incliner les labels de l'axe X pour meilleure lisibilité
            yaxis=dict(range=[0, 200])  # Modifier l'axe des ordonnées de 0 à 200 pour pouvoir comparer les graphiques
        )

        # Positionner les textes en dehors des barres
        fig_jour_ferie.update_traces(
            textposition='outside',
            selector=dict(type='bar')
        )

        #____________________ HOLIDAYS
        # Data Jours VACANCES
        data_holidays = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\data_holidays.csv') 

        # Graphique JOURS FERIES
        fig_holidays = px.bar(
            data_holidays,
            x='Heure',
            y='Comptage_horaire',
            text=round(data_holidays["Comptage_horaire"], 0),
            color='Tranche_horaire',
            color_discrete_map={
                '1-Créneau 6h - 10h': "Darkblue",
                '2-Créneau 11h - 15h': "LightBlue",
                '3-Créneau 16h - 20h': 'Blue',
                '4-Créneau 21h - 23h': 'Pink',
                '5-Créneau minuit - 5h (nuit)': 'DeepPink'
            }
        )

        # Mettre à jour la disposition du graphique
        fig_holidays.update_layout(
            showlegend=True,  
            legend=dict(
                orientation="h",  # Orientation horizontale
                yanchor="top",
                y=-0.2,  # Positionner la légende en bas
                xanchor="center",
                x=0.5
            ),
            xaxis_title='Heure',
            yaxis_title='Comptage Horaire',
            title_text='Moyenne des passages - Vacances scolaires toutes zones',
            title_x=0,
            title_font_size=20,
            xaxis_tickangle=-45,  # Incliner les labels de l'axe X pour meilleure lisibilité
            yaxis=dict(range=[0, 200])  # Modifier l'axe des ordonnées de 0 à 200 pour pouvoir comparer les graphiques
        )

        # Mettre à jour les traces pour positionner les textes en dehors des barres
        fig_holidays.update_traces(
            textposition='outside',
            selector=dict(type='bar')
        )

        # Afficher les graphiques côte à côte dans Streamlit
        col1, col2 = st.columns(2)

        with col2:
            st.plotly_chart(fig_jour_ferie, use_container_width=True)

        with col1:
            st.plotly_chart(fig_holidays, use_container_width=True)


#___________________Machine Learning 1
elif page == pages[4]: 
    # Tableau des performances
    modele_et_parametrage = [
        {
            "Type": "Modèle d'arbres <br> Gradient Boosting",
            "Modèle": "XGBoost",
            "Performance": "robuste",
            "Alternative retenue": "Par défaut",
            "Alternative testée": "1) Par défaut - robuste<br> 2) Hyperparamètres optimisés - correct"
        },
        {
            "Type": "Modèle d'arbres <br> Gradient Boosting",
            "Modèle": "LightGBM",
            "Performance": "robuste",
            "Alternative retenue": "Hyperparamètres<br> RFE",
            "Alternative testée": "1) Par défaut - correct<br> 2) Hyperparamètres optimisés - robuste<br> 2) SelectKBest - médiocre<br> 3) RFE - robuste<br> 4) ACP (non pertinent)"
        },
        {
            "Type": "Modèle d'arbres <br>simple",
            "Modèle": "DecisionTreeRegressor",
            "Performance": "robuste",
            "Alternative retenue": "Par défaut<br> selected variables",
            "Alternative testée": "1) Par défaut - robuste <br> 2) Hyperparamètres optimisés - médiocre<br> 3) 36 variables sélectionnées (95% variance) - correct <br> 4) optimisation hyperparamètre avec approche bayesienne Optuna - médiocre"
        },
        {
            "Type": "Modèle d'arbres <br> aléatoire",
            "Modèle": "Random Forest",
            "Performance": "robuste",
            "Alternative retenue": "Hyperparamètres<br>  Isolation Forest",
            "Alternative testée": "1) Hyperparamètres optimisés - robuste<br>  2) 65 variables sélectionnées (95% variance) - correct<br> 3) Isolation Forest - robuste <br> 4) Transformation logarithmique - médiocre<br>"
        },

        {
            "Type": "Régression pénalisée",
            "Modèle": "RidgeCV (cv5)",
            "Performance": "médiocre",
            "Alternative retenue": "/",
            "Alternative testée": "/"
        },
        {
            "Type": "Régression pénalisée",
            "Modèle": "ElasticNetCV (cv5)",
            "Performance": "médiocre",
            "Alternative retenue": "/",
            "Alternative testée": "/"
        },
        {
            "Type": "Régression pénalisée",
            "Modèle": "LassoCV (cv5)",
            "Performance": "médiocre",
            "Alternative retenue": "/",
            "Alternative testée": "/"
        }

    ]
    # Fonction pour mettre en gras les occurrences de "robuste"
    def color_cellule(text):
        if "robuste" in text:
            return f"<span style='color: white; background-color: green; padding: 8px; display: block;'>{text}</span>"
        elif "médiocre" in text:
            return f"<span style='color: white; background-color: red; padding: 8px; display: block;'>{text}</span>"
        elif "correct" in text:
            return f"<span style='color: white; background-color: blue; padding: 8px; display: block;'>{text}</span>"
        else:
            return f"<span style='padding: 8px; display: block;'>{text}</span>"
    def color_text(text):
        text = text.replace("robuste", "<strong style='color: green;'>robuste</strong>")
        text = text.replace("médiocre", "<strong style='color: red;'>médiocre</strong>")
        text = text.replace("correct", "<strong style='color: blue;'>correct</strong>")
        return text
    # Créer une table HTML
    table_html = """
    <div style="border: 1px solid #ddd; max-height: 400px; overflow-y: scroll;">
        <table style="border-collapse: collapse; width: 100%;">
            <thead>
                <tr style="background-color: #f2f2f2;">
                    <th style="padding: 8px; border: 1px solid #ddd;">Type</th>
                    <th style="padding: 8px; border: 1px solid #ddd;">Modèle</th>
                    <th style="padding: 8px; border: 1px solid #ddd;">Performance</th>
                    <th style="padding: 8px; border: 1px solid #ddd;">Alternative retenue</th>
                    <th style="padding: 8px; border: 1px solid #ddd;">Alternative testée</th>
                </tr>
            </thead>
            <tbody>
    """
    # Ajouter les lignes du tableau
    for item in modele_et_parametrage:
        table_html += "<tr>"
        table_html += f"<td style='padding: 8px; border: 1px solid #ddd;'>{item['Type']}</td>"
        table_html += f"<td style='padding: 8px; border: 1px solid #ddd;'>{item['Modèle']}</td>"
        table_html += f"<td style='padding: 8px; border: 1px solid #ddd;'>{color_cellule(item['Performance'])}</td>"
        table_html += f"<td style='padding: 8px; border: 1px solid #ddd;'>{item['Alternative retenue']}</td>"
        table_html += f"<td style='padding: 8px; border: 1px solid #ddd;'>{color_text(item['Alternative testée'])}</td>"
        table_html += "</tr>"
    table_html += """
        </tbody>
    </table>
    """
    # Afficher le tableau HTML 
    st.subheader("Tableau des Performances des Modèles")
    st.markdown(table_html, unsafe_allow_html=True)

    #________Charger les données
    @st.cache_data 
    def load_data():
        df_pre = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_pre_modelisation.csv') # Id_Compteur
        return df_pre

     

    #________Charger les modèles enregistrés avec JOBLIB
    @st.cache_data 
    def load_models():
        RandomForest1_model = joblib.load('RandomForestPipe_compteursParis.pkl')
        TreeRegressor_model = joblib.load('treeRegressor1_compteursParis.pkl')
        XGB_model = joblib.load('xgb_compteursParis.pkl')
        LGB1_model = joblib.load('lgb_compteursParis1.pkl')
        return RandomForest1_model, TreeRegressor_model, XGB_model, LGB1_model


    #________Séparer le jeu de données en features (X) et target (y) 
    df_pre = load_data()

    # Définir une liste vide pour stocker les numéros de semaine sélectionnés
    selected_weeks = []  
    for week in range(4, 53, 4): # Boucler sur les multiples en 4 de 4 à 52 inclus
        selected_weeks.append(week)

    y = df_pre['Comptage_horaire']
    X = df_pre.drop('Comptage_horaire', axis=1)


    # Créer un masque booléen pour les lignes à conserver dans le train et le test
    test_mask = df_pre['Num_semaine'].isin(selected_weeks)
    X_test = X[test_mask]
    y_test = y[test_mask]  

    print(f"Taille de l'ensemble de test : {len(X_test)} ({round((len(X_test)/len(df_pre)) * 100)} %)") 

    # Fonction pour calculer le RMSLE
    def rmsle(y_true, y_pred):
        return np.sqrt(np.mean((np.log1p(np.clip(y_pred, 0, None)) - np.log1p(y_true)) ** 2))
    rmsle_scorer = make_scorer(rmsle)

    #-----------------------------------------Faire les prédictions avec chaque modèle
    RandomForest1_model, TreeRegressor_model, XGB_model, LGB1_model = load_models()

    y_pred_Lgb1 = LGB1_model.predict(X_test)
    y_pred_Xgb = XGB_model.predict(X_test)
    y_pred_TreeRegressor = TreeRegressor_model.predict(X_test)
    y_pred_RandomForest = RandomForest1_model.predict(X_test)
   
    #-----------------------------------------Calculer les métriques pour chaque modèle
    #_________ LGBX
    metrics_Lgb1 = pd.DataFrame({
        'R²': [r2_score(y_test, y_pred_Lgb1)],
        'RMSLE': [rmsle(y_test, y_pred_Lgb1)],
        'RMSE': [mean_squared_error(y_test, y_pred_Lgb1, squared=False)],
        'MSE': [mean_squared_error(y_test, y_pred_Lgb1)],
        'MAE': [mean_absolute_error(y_test, y_pred_Lgb1)],
    }, index=['Prédiction'])

    #_________ XGB
    metrics_Xgb = pd.DataFrame({
        'R²': [r2_score(y_test, y_pred_Xgb)],
        'RMSLE': [rmsle(y_test, y_pred_Xgb)],
        'RMSE': [mean_squared_error(y_test, y_pred_Xgb, squared=False)],
        'MSE': [mean_squared_error(y_test, y_pred_Xgb)],
        'MAE': [mean_absolute_error(y_test, y_pred_Xgb)],
    }, index=['Prédiction'])

    #_________ TreeRegressor
    metrics_TreeRegressor = pd.DataFrame({
        'R²': [r2_score(y_test, y_pred_TreeRegressor)],
        'RMSLE': [rmsle(y_test, y_pred_TreeRegressor)],
        'RMSE': [mean_squared_error(y_test, y_pred_TreeRegressor, squared=False)],
        'MSE': [mean_squared_error(y_test, y_pred_TreeRegressor)],
        'MAE': [mean_absolute_error(y_test, y_pred_TreeRegressor)],
    }, index=['Prédiction'])

    #_________RandomForest
    metrics_RandomForest = pd.DataFrame({
        'R²': [r2_score(y_test, y_pred_RandomForest)],
        'RMSLE': [rmsle(y_test, y_pred_RandomForest)],
        'RMSE': [mean_squared_error(y_test, y_pred_RandomForest, squared=False)],
        'MSE': [mean_squared_error(y_test, y_pred_RandomForest)],
        'MAE': [mean_absolute_error(y_test, y_pred_RandomForest)],
    }, index=['Prédiction'])


    #-----------------------------------------Créer les dataframes
    #_________ LGBX
    # DataFrame valeurs réelles et prévues + ajouter les Address ET Fusionner les informations d'adresse avec comparison_df_Lgb1
    comparison_df_Lgb1  = pd.DataFrame({
        'Comptages_réels': y_test.values,
        'Comptages_prédits': y_pred_Lgb1.astype(int), 
        'Id_Compteur' : X_test['Id_Compteur'], 
        'Heure' : X_test['Heure'], 
     }, index=X_test.index).merge(pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv'), on='Id_Compteur', how='left')
    
    #_________ XGB
    comparison_df_Xgb = pd.DataFrame({
        'Comptages_réels': y_test.values,
        'Comptages_prédits': y_pred_Xgb.astype(int), 
        'Id_Compteur' : X_test['Id_Compteur'], 
        'Heure' : X_test['Heure'], 
     }, index=X_test.index).merge(pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv'), on='Id_Compteur', how='left')
    
    #_________ TreeRegressor
    comparison_df_TreeRegressor = pd.DataFrame({
        'Comptages_réels': y_test.values,
        'Comptages_prédits': y_pred_TreeRegressor.astype(int), 
        'Id_Compteur' : X_test['Id_Compteur'], 
        'Heure' : X_test['Heure'], 
     }, index=X_test.index).merge(pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv'), on='Id_Compteur', how='left')
    
    #_________RandomForest
    comparison_df_RandomForest = pd.DataFrame({
        'Comptages_réels': y_test.values,
        'Comptages_prédits': y_pred_RandomForest.astype(int), 
        'Id_Compteur' : X_test['Id_Compteur'], 
        'Heure' : X_test['Heure'], 
     }, index=X_test.index).merge(pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv'), on='Id_Compteur', how='left')

    #-----------------------------------------Filtrer et calculer les moyennes pour chaque modèle
    #_________ LGBX
    df_compteurs_prevision_Lgb1 = comparison_df_Lgb1.groupby(['Heure']).agg({
        'Comptages_réels': 'mean',
        'Comptages_prédits': 'mean'
    }).reset_index()
    

    #_________ XGB
    df_compteurs_prevision_Xgb = comparison_df_Xgb.groupby(['Heure']).agg({
        'Comptages_réels': 'mean',
        'Comptages_prédits': 'mean'
    }).reset_index()

    #_________ TreeRegressor
    df_compteurs_prevision_TreeRegressor = comparison_df_TreeRegressor.groupby(['Heure']).agg({
        'Comptages_réels': 'mean',
        'Comptages_prédits': 'mean'
    }).reset_index()

    #_________RandomForest
    df_compteurs_prevision_RandomForest = comparison_df_RandomForest.groupby(['Heure']).agg({
        'Comptages_réels': 'mean',
        'Comptages_prédits': 'mean'
    }).reset_index()

    #----------------------------------------- Faire une synthèse 

    # Regrouper toutes les métriques dans un seul DataFrame
    all_metrics = pd.concat([metrics_Lgb1, metrics_Xgb, metrics_TreeRegressor, metrics_RandomForest], keys=['LightGBM', 'XGBoost', 'TreeRegressor', 'RandomForest'])

    # Trouver le meilleur R²
    best_r2_model = all_metrics.loc[all_metrics['R²'].idxmax()]

    # Trouver la meilleure métriques
    best_rmsle_model = all_metrics.loc[all_metrics['RMSLE'].idxmin()]
    best_rmse_model = all_metrics.loc[all_metrics['RMSE'].idxmin()]
    best_mse_model = all_metrics.loc[all_metrics['MSE'].idxmin()]
    best_mae_model = all_metrics.loc[all_metrics['MAE'].idxmin()]

    # Affichage
    st.write("### Métriques des Modèles")
    st.write(all_metrics)

    st.write("### Conclusion")

    # Commentaire sur le meilleur R²
    st.write(f"Le modèle avec le meilleur R² est : **{best_r2_model.name}** avec un R² de {best_r2_model['R²']:.4f}.")

    # Commentaire sur les métriques de précision
    st.write(f"Le modèle avec les meilleures performances en termes de précision RMSLE est : **{best_rmsle_model.name[0]}**  avec un score de {best_rmsle_model['RMSLE']:.4f}.\n"
             "La RMSLE est particulièrement utile lorsque on analyse davantage les erreurs proportionnelles plutôt que des erreurs absolues. Cela signifique de meilleures prédictions relatives pour les petites valeurs.")

    st.write(f"Le modèle avec les meilleures performances en termes de précision RMSE, MSE, MAE est : **{best_rmse_model.name[0]} ({best_rmse_model['RMSE']:.4f})**, **{best_mse_model.name[0]} ({best_mse_model['MSE']:.4f})**, **{best_mae_model.name[0]} ({best_mae_model['MAE']:.4f})**.\n" 
            "Le modèle est globalement performant en terme d'erreurs absolues, et robuste aux grandes valeurs.")

    # Commentaire général
    st.write("""
    En général, le modèle avec le R² le plus élevé est considéré comme ayant la meilleure capacité à expliquer la variance des données.\n 
    Cependant, les métriques de précision (RMSE, MSE, MAE) donnent également des indications importantes sur la qualité des prédictions.\n
    Le modèle qui a les métriques les plus basses (c'est-à-dire, la moyenne des métriques les plus petites) en combinaison avec un R² élevé est souvent considéré comme le plus fiable.
            
            
    """)


    st.write('----------')

   #_________________GRAPHIQUE_________________#

    # Créer le graphique pour LightGBM
    fig_Lgb1 = px.line(df_compteurs_prevision_Lgb1, x='Heure', y=['Comptages_réels', 'Comptages_prédits'], 
                    labels={'value': 'Comptages', 'variable': 'Légende'},
                    title=f'Comptages réels et prédits (LightGBM)')


    # Créer le graphique pour XGBoost
    fig_Xgb = px.line(df_compteurs_prevision_Xgb, x='Heure', y=['Comptages_réels', 'Comptages_prédits'], 
                    labels={'value': 'Comptages', 'variable': 'Légende'},
                    title=f'Comptages réels et prédits (XGBoost)')
    
    # Créer le graphique pour TreeRegressor
    fig_TreeRegressor = px.line(df_compteurs_prevision_TreeRegressor, x='Heure', y=['Comptages_réels', 'Comptages_prédits'], 
                    labels={'value': 'Comptages', 'variable': 'Légende'},
                    title=f'Comptages réels et prédits(/br) (XGBoost)')  
    
     # Créer le graphique pour RandomForestRegressor
    fig_RandomForest = px.line(df_compteurs_prevision_RandomForest, x='Heure', y=['Comptages_réels', 'Comptages_prédits'], 
                    labels={'value': 'Comptages', 'variable': 'Légende'},
                    title=f'Comptages réels et prédits(/br) (XGBoost)')  


    #___________________________ Affichage des GRAPHIQUES

    
    col1, col2 = st.columns(2)
    with col1:
        st.header("LGBM Metrics")
        st.dataframe(metrics_Lgb1)
        st.plotly_chart(fig_Lgb1)
    with col2:
        st.header("XGB Metrics")
        st.dataframe(metrics_Xgb)
        st.plotly_chart(fig_Xgb)
    st.write("")  

    col3, col4 = st.columns(2)
    with col3:
        st.header("Tree Regressor Metrics")
        st.dataframe(metrics_TreeRegressor)
        st.plotly_chart(fig_TreeRegressor)
    with col4:
        st.header("RandomForest Metrics")
        st.dataframe(metrics_RandomForest)
        st.plotly_chart(fig_RandomForest)

#___________________Machine Learning 2
elif page == pages[5]: 
    #________Charger les données
    @st.cache_data 
    def load_data():
        df_pre = pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_pre_modelisation.csv') # Id_Compteur
        lien_photos= pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\lien_photos.csv') #Id_Compteur Lien_image
        nom_compteur= pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv') #Id_Compteur Address
        return df_pre, lien_photos, nom_compteur
    
    #________Charger les modèles enregistrés avec JOBLIB
    @st.cache_data 
    def load_model():
        return joblib.load('treeRegressor1_compteursParis.pkl')

    LGB1_model  = load_model()

    #________Séparer le jeu de données en features (X) et target (y) 
    df_pre, lien_photos, nom_compteur = load_data()

    # Définir une liste vide pour stocker les numéros de semaine sélectionnés
    selected_weeks = []  
    for week in range(4, 53, 4): # Boucler sur les multiples en 4 de 4 à 52 inclus
        selected_weeks.append(week)

    y = df_pre['Comptage_horaire']
    X = df_pre.drop('Comptage_horaire', axis=1)

    # Créer un masque booléen pour les lignes à conserver dans le train et le test
    test_mask = df_pre['Num_semaine'].isin(selected_weeks)
    X_test = X[test_mask]
    y_test = y[test_mask]  

    print(f"Taille de l'ensemble de test : {len(X_test)} ({round((len(X_test)/len(df_pre)) * 100)} %)")  
    


    # Fonction pour calculer le RMSLE
    def rmsle(y_true, y_pred):
        return np.sqrt(np.mean((np.log1p(np.clip(y_pred, 0, None)) - np.log1p(y_true)) ** 2))
    rmsle_scorer = make_scorer(rmsle)

    #-----------------------------------------Faire les prédictions avec chaque modèle
    y_pred_Lgb1 = LGB1_model.predict(X_test)

    #-----------------------------------------Calculer les métriques pour chaque modèle
    #_________ LGBX
    metrics_Lgb1 = pd.DataFrame({
        'R²': [r2_score(y_test, y_pred_Lgb1)],
        'RMSLE': [rmsle(y_test, y_pred_Lgb1)],
        'RMSE': [mean_squared_error(y_test, y_pred_Lgb1, squared=False)],
        'MSE': [mean_squared_error(y_test, y_pred_Lgb1)],
        'MAE': [mean_absolute_error(y_test, y_pred_Lgb1)],
    }, index=['Prédiction'])
    

    #-----------------------------------------Créer les dataframes   
    #_________ LGBX
    # DataFrame valeurs réelles et prévues + ajouter les Address ET Fusionner les informations d'adresse avec comparison_df_Lgb1
    comparison_df_Lgb1  = pd.DataFrame({
        'Comptages_réels': y_test.values,
        'Comptages_prédits': y_pred_Lgb1.astype(int), 
        'Id_Compteur' : X_test['Id_Compteur'], 
        'Heure' : X_test['Heure'], 
        'District':X_test['District']
     }, index=X_test.index)


    comparison_df_Lgb1 = comparison_df_Lgb1.merge(nom_compteur, on='Id_Compteur', how='left').merge(lien_photos, on='Id_Compteur',how='left')

    #-----------------------------------------Filtrer et calculer les moyennes pour chaque modèle
    #_________ LGBX
    df_compteurs_prevision_Lgb1 = comparison_df_Lgb1.groupby(['Id_Compteur', 'Address','Heure']).agg({
        'Comptages_réels': 'mean',
        'Comptages_prédits': 'mean'
    }).reset_index()






    #---------------------- Créer les groupes de fréquence compteurs
    df_groupe = pd.DataFrame(comparison_df_Lgb1[['Id_Compteur','District','Comptages_réels']])
    df_groupe['Mean_Comptages'] = df_groupe.groupby('Id_Compteur')['Comptages_réels'].transform('mean') # Faire la moyenne
    df_groupe['Group'] = pd.qcut(df_groupe['Mean_Comptages'], 4, labels=['groupe 1 (Faible affluence)', 'groupe 2 (Moyenne affluence)', 'groupe 3 (Affluence forte)', 'groupe 4 (Affluence très élevée)']) # Créer les 4 groupes par moyenne
    df_groupe = df_groupe.drop(columns='Comptages_réels').drop_duplicates().reset_index(drop=True).merge(pd.read_csv('C:\\Users\\La Foune\\Documents\\Streamlit\\df_compteur_unique.csv'), on='Id_Compteur', how='left') # Supprimer les colonnes inutiles, ajouter Address

    # Calculer les statistiques pour chaque groupe
    stats = {}
    for groupe in df_groupe['Group'].unique():
        group_data = df_groupe[df_groupe['Group'] == groupe]
        min_value = group_data['Mean_Comptages'].min()
        max_value = group_data['Mean_Comptages'].max()
        mean_value = group_data['Mean_Comptages'].mean()
        stats[groupe] = {'min': min_value, 'max': max_value, 'mean': mean_value}

    ordre_groupes = ['groupe 1 (Faible affluence)', 'groupe 2 (Moyenne affluence)', 'groupe 3 (Affluence forte)', 'groupe 4 (Affluence très élevée)']

    # Case à cocher 
    st.write('Filtrer les compteurs par densité de passage :')
    groupes_selectionnes = []

    # Afficher les cases à cocher avec les statistiques
    for groupe in ordre_groupes:
        min_val = stats[groupe]['min']
        max_val = stats[groupe]['max']
        mean_val = stats[groupe]['mean']
        
        # Afficher les cases à cocher avec les statistiques intégrées
        if st.checkbox(
            f'Afficher {groupe} - min.: {min_val:.2f}, max.: {max_val:.2f}, moyenne: {mean_val:.2f})', 
            value=True ):
            groupes_selectionnes.append(groupe)

    # Filtrer les données en fonction des groupes sélectionnés
    if groupes_selectionnes:
        df_selectionne = df_groupe[df_groupe['Group'].isin(groupes_selectionnes)]
    else:
        df_selectionne = df_groupe


    st.write('----------')


    # Liste déroulante pour sélectionner et filtrer un compteur
    nom_compteur_selectionne = st.selectbox('Sélectionnez un compteur du groupe choisi', df_selectionne['Address'].unique(), key='selectbox_compteur')

    # Récupérer le numéro de district associé au compteur sélectionné
    district_selectionne = df_groupe[df_groupe['Address'] == nom_compteur_selectionne]['District'].iloc[0]

    groupe_compteur =  df_groupe[df_groupe['Address'] == nom_compteur_selectionne]['Group'].iloc[0]

    # Filtrer les données en fonction du compteur sélectionné
    df_selectionne_Lgb1 = df_compteurs_prevision_Lgb1[df_compteurs_prevision_Lgb1['Address'] == nom_compteur_selectionne]

    st.write('----------')
    st.header("Caractéristiques de l'arrondissement")
    col1, col2 = st.columns(2)
    with col1:
        # Filtrer les données pour l'arrondissement du compteur sélectionné
        df_arrondissement = comparison_df_Lgb1[comparison_df_Lgb1['District'] == district_selectionne]
        data_arrondissement = df_arrondissement.groupby('Heure', as_index=False).agg({'Comptages_réels': 'mean'})

        # Créer le graphique en ligne
        fig_arrondissement = px.line(
            data_arrondissement, 
            x='Heure', 
            y='Comptages_réels',
            title=f'Comptage horaire moyen pour l\'arrondissement {district_selectionne}',
            markers=True,
            height=300,  
            width=500 
            
            )
        
        fig_arrondissement.update_traces(mode='lines+markers', 
                                        text=[f'Moyenne: {round(val)}' for val in data_arrondissement['Comptages_réels']], 
                                        textposition='top right')
        
        # Afficher le graphique
        st.plotly_chart(fig_arrondissement)

    with col2:
        data_districts= comparison_df_Lgb1.groupby(['District'], as_index=False).agg({'Comptages_réels': 'mean'})
        data_districts = data_districts.sort_values(by='Comptages_réels', ascending = False)
        data_districts.reset_index(drop=True, inplace=True)
        
        # Ajouter une colonne pour catégoriser en 4 groupes de fréquentation
        labels = ['groupe 1 (Faible affluence)', 'groupe 2 (Moyenne affluence)', 'groupe 3 (Affluence forte)', 'groupe 4 (Affluence très élevée)']
        data_districts['Fréquentation'] = pd.qcut(data_districts['Comptages_réels'], q=4, labels=labels)
        
        # Calculer la position du district sélectionné dans le classement
        classement = data_districts.index[data_districts['District'] == district_selectionne].tolist()[0] + 1
        
        # Filtrer les données pour obtenir uniquement le district sélectionné
        data_district_selectionne = data_districts[data_districts['District'] == district_selectionne]
        moyenne_comptages = data_district_selectionne['Comptages_réels'].iloc[0]
        frequentation = data_district_selectionne['Fréquentation'].iloc[0]
        
        st.markdown(f"Le compteur sélectionné **{nom_compteur_selectionne}** est situé dans l'**arrondissement {district_selectionne}**.", unsafe_allow_html=True)
        st.markdown(f"**Moyenne des comptages horaires de l'arrondissement :<br>** {round(moyenne_comptages, 2)} par jour", unsafe_allow_html=True)
        st.markdown(f"**Classement de l'arrondissement :**<br> {classement} sur {len(data_districts)} (pas de compteurs arrdts 6 et 9).<br> Arrondissement catégorisé comme appartenant au **{frequentation}.**", unsafe_allow_html=True)
        

        # Dictionnaire avec les descriptions des arrondissements
        descriptions = {
            1: "Arrondissement 1 (Louvre, Palais Royal) : Zone touristique majeure avec le Louvre et d'autres attractions. Forte concentration de commerces et de bureaux.",
            2: "Arrondissement 2 (Bourse, Montorgueil) : Zone d’affaires avec une concentration élevée de bureaux, de commerces, et plusieurs stations Vélib’. L'activité économique est intense, avec une grande densité de bureaux et de commerces.",
            3: "Arrondissement 3 (Le Marais) : Quartier historique avec de nombreux musées, boutiques et restaurants. Vie nocturne animée.",
            4: "Arrondissement 4 (Le Marais, Île de la Cité) : Quartier touristique avec le Marais et l'Île de la Cité, riche en attractions culturelles et nocturnes. Vie culturelle animée avec de nombreux restaurants, bars, et musées.",
            5: "Arrondissement 5 (Latin Quarter) : Quartier universitaire, touristique avec des sites comme la Sorbonne. Vie étudiante animée.",
            6: "Arrondissement 6 (Saint-Germain-des-Prés) : Quartier chic avec de nombreux cafés, boutiques et galeries. Présence de nombreux parcs et espaces verts.",
            7: "Arrondissement 7 (Tour Eiffel, Invalides) : Zone touristique avec la Tour Eiffel et les Invalides. Beaucoup de visiteurs et de touristes.",
            8: "Arrondissement 8 (Champs-Élysées) : Zone commerciale et touristique avec les Champs-Élysées. Présence de nombreux hôtels et boutiques.",
            9: "Arrondissement 9 (Opéra, Grands Boulevards) : Quartier commerçant avec des grands magasins et des théâtres. Zone dynamique avec beaucoup de bureaux.",
            10: "Arrondissement 10 (Gare du Nord, Gare de l'Est, Canal Saint-Martin) : Nœud de transport majeur avec les gares du Nord et de l'Est, et un quartier dynamique le long du Canal Saint-Martin. Fort flux de voyageurs et présence de pistes cyclables.",
            11: "Arrondissement 11 (Bastille, Oberkampf) : Quartier résidentiel animé avec des rues commerçantes, des bars, et des restaurants. Fort dynamisme nocturne et activités culturelles.",
            12: "Arrondissement 12 (Reuilly, Bercy) : Quartier résidentiel avec le parc de Bercy et le Bois de Vincennes. Zones commerciales et résidentielles.",
            13: "Arrondissement 13 (Gobelins, Butte-aux-Cailles) : Quartier résidentiel avec des influences artistiques et des espaces verts. Zone populaire pour les familles.",
            14: "Arrondissement 14 (Montparnasse) : Quartier résidentiel avec des écoles et des bureaux. Présence d'activités culturelles.",
            15: "Arrondissement 15 (Vaugirard) : Quartier principalement résidentiel avec des espaces verts comme le parc André Citroën.",
            16: "Arrondissement 16 (Passy) : Quartier résidentiel chic avec des espaces verts et des institutions éducatives.",
            17: "Arrondissement 17 (Batignolles) : Quartier en transformation avec des espaces verts et une zone résidentielle animée. En cours de développement des infrastructures cyclables.",
            18: "Arrondissement 18 (Montmartre) : Quartier historique et touristique avec Montmartre. Vie culturelle riche.",
            19: "Arrondissement 19 (Buttes-Chaumont) : Quartier avec le parc des Buttes-Chaumont et une grande diversité culturelle.",
            20: "Arrondissement 20 (Ménilmontant) : Quartier populaire avec une vie nocturne animée et des espaces culturels."
        }

        # Afficher la description correspondant à l'arrondissement sélectionné
        description = descriptions.get(district_selectionne, "Description non disponible pour cet arrondissement.")
        st.markdown(f"**Caractéristiques principales de l'arrondissement :**<br>{description}", unsafe_allow_html=True)




    st.write('----------')
    st.header("Constat général sur l'activité cycliste")
    st.markdown("""
        - Le réseau REVe concentre 9% des compteurs vélo et 63% des passages.
        - Les bois concentrent uniquement 1% des compteurs et 21% des comptages. A  noter la mauvaise représentation en raison de l'absence de compteurs.
        - 86,6% des compteurs sont situés sur une voie à 50 km/h, néanmoins, la fréquentation des vélos est plus forte dans les voies/zones à 30 km/h avec environ 30% des passages, et 14% dans les aires piétonnes (1,22% des compteurs).
        - 37% des compteurs se situent sur un aménagement bidirectionnel et représentent 48% des passages.
        """)

    st.write('----------')
    #_________________GRAPHIQUE_________________#

    # Créer le graphique pour LGBX
    fig_Lgb1 = px.line(df_selectionne_Lgb1, x='Heure', y=['Comptages_réels', 'Comptages_prédits'], 
                    labels={'value': 'Comptages', 'variable': 'Légende'},
                    title=f'Comptages réels et prédits(/br){nom_compteur_selectionne} ')  
    
    #___________________________ Affichage 

    col3, col4 = st.columns(2)
    with col3:
        st.header("Métrique LGBX")
        st.dataframe(metrics_Lgb1)
        st.plotly_chart(fig_Lgb1)
        st.write("")
        st.header("Caractéristiques du compteur")

        # Filtrer les données pour le compteur sélectionné
        # Créer un nouveau DataFrame avec les colonnes spécifiées et supprimer les doublons
        columns_to_keep = ['Id_Compteur', 'typologie_simple', 'amenagement_bidirectionnel', 'regime_vitesse', 'sens_velo', 'bois', 'couloir_bus', 'reseau_REVe']
        df_typologie = df_pre[columns_to_keep].drop_duplicates()
        
        # Obtenir les Id_Compteur pour le compteur sélectionné
        id_compteur_selectionne = nom_compteur[nom_compteur['Address'] == nom_compteur_selectionne]['Id_Compteur'].iloc[0]
        df_typologie_filtered = df_typologie[df_typologie['Id_Compteur'] == id_compteur_selectionne]

        typologie_simple = df_typologie_filtered['typologie_simple'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        amenagement_bidirectionnel = df_typologie_filtered['amenagement_bidirectionnel'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        regime_vitesse = df_typologie_filtered['regime_vitesse'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        sens_velo = df_typologie_filtered['sens_velo'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        bois = df_typologie_filtered['bois'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        couloir_bus = df_typologie_filtered['couloir_bus'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        reseau_REVe = df_typologie_filtered['reseau_REVe'].iloc[0] if not df_typologie_filtered.empty else 'Inconnu'
        
        # Creating the dataframe
        caracteristique_compteurs = {
            'Nom du compteur': [nom_compteur_selectionne],
            'Affluence du compteur': [groupe_compteur],
            'Arrondissement': [district_selectionne],
            'Affluence de l\'arrondissement': [frequentation],
            'Typologie': [typologie_simple],
            'Aménagement bidirectionnel': ['Oui' if amenagement_bidirectionnel == 1 else 'Non'],
            'Régime de vitesse': [regime_vitesse],
            'Sens vélo': [sens_velo],
            'Présence de bois': ['Oui' if bois == 1 else 'Non'],
            'Couloir de bus': [couloir_bus],
            'Réseau REVe': ['Oui' if reseau_REVe == 1 else 'Non']
        }

        caracteristique_compteurs = pd.DataFrame(caracteristique_compteurs).T
        caracteristique_compteurs.columns = ['Valeur']
        caracteristique_compteurs.reset_index(inplace=True)
        caracteristique_compteurs.columns = ['Caractéristiques', 'Valeur']

        # Appliquer le style pour mettre en gras la colonne "Caractéristiques"
        def highlight_col(val):
            return 'font-weight: bold'

        caracteristique_compteurs_styled = caracteristique_compteurs.style.applymap(highlight_col, subset=['Caractéristiques'])

        # Afficher le tableau stylé
        st.write(caracteristique_compteurs_styled.hide(axis=0).to_html(index=False), unsafe_allow_html=True)



        

    
    with col4:
        st.header("Image compteur")
        lien_image = lien_photos[lien_photos['Id_Compteur'].isin(df_selectionne_Lgb1['Id_Compteur'])].set_index('Id_Compteur')['Lien_image']
        image_url = lien_image.loc[df_selectionne_Lgb1['Id_Compteur'].values[0]] if not lien_image.empty else None
        
        # Afficher l'image
        if image_url:
            st.image(image_url, caption='Image du compteur', width=700)
            st.markdown(
                f"""
                <style>
                .stImage img {{
                    height: 900px;  
                     
                }}
                </style>
                """,
                unsafe_allow_html=True
            )
        else:
            st.write("Aucune image disponible pour le compteur sélectionné.")
        
        #Afficher la carte
        st.header("Carte")
 
        
        @st.cache_data 
        def load_data():
            districts = gpd.read_file('C:\\Users\\La Foune\\Documents\\Streamlit\\districts.geojson') 
            df_reseau_lambert_carte = gpd.read_file('C:\\Users\\La Foune\\Documents\\Streamlit\\df_reseau_lambert_carte.geojson')         
            return districts, df_reseau_lambert_carte
        
        districts,df_reseau_lambert_carte = load_data()

        df_pre = df_pre.merge(nom_compteur, on='Id_Compteur', how='left')
        df_pre.rename(columns={'nom_compteur': 'Address'}, inplace=True)



        compteur_selectionne = df_pre[df_pre['Address']==nom_compteur_selectionne].iloc[0]
        # Création de la carte
        
        paris_map2 = folium.Map(location=[compteur_selectionne["Latitude"], compteur_selectionne["Longitude"]], zoom_start=15, min_zoom=10, max_zoom=20, control_scale=True)
        #paris_map2 = folium.Map(location=[48.856578, 2.351828], zoom_start=12, min_zoom=10, max_zoom=20, control_scale=True)
        # Calculer la moyenne horaire par arrondissement
        district_mean = df_pre.groupby(["District"], as_index=False)["Comptage_horaire"].mean()
        print(district_mean)

        # Obtenir les points géographiques des districts
        districts_data = districts.merge(district_mean, left_on="District", right_on="District", how="left")
        districts_data['index'] = districts_data['District']
        districts_data = districts_data.sort_values('index')
        districts_data = districts_data.set_index('index')

        # Pas de compteur dans arrondissement 6 et 9 - pas de données --> ajout des 2 lignes avec None (car couleur jaune pale par défaut)
        color_mean_dict = pd.concat([district_mean,pd.DataFrame({"District":["6","9"],"Comptage_horaire":[None,None]})])
        color_mean_dict.District = color_mean_dict.District.astype(int)
        color_mean_dict = color_mean_dict.sort_values(['District'])
        color_mean_dict = color_mean_dict.set_index("District")["Comptage_horaire"] # avoir l'index identique à District pour avoir la cohérence avec districts_data

        ######################### Mise en place des éléments de la carte

        # # Créer des FeatureGroups pour chaque catégorie de données
        fg0 = folium.FeatureGroup(name='Arrondissements', show=False) # Arrondissements
        fg1 = folium.FeatureGroup(name='Réseau REVe et bois', show=True)
        fg2 = folium.FeatureGroup(name='Marker avec couleur pour typologie réseau', show=True)
        fg7 = folium.FeatureGroup(name='Marker comopteurs les 10 tops et flops', show=True)
        fg6 = folium.FeatureGroup(name='Fréquentation', show=True) #CircleMarker

        typ1 = folium.FeatureGroup(name='Bandes cyclables', show=False)
        typ2 = folium.FeatureGroup(name='Autres itinéraires', show=False)
        typ3 = folium.FeatureGroup(name='Pistes cyclables', show=False)
        typ4 = folium.FeatureGroup(name='Couloirs de bus', show=False)

        ###########################
        # Définir le colormap avec les différentes palettes choix orange
        colormap = linear.YlOrRd_09.scale(district_mean.Comptage_horaire.min(), district_mean.Comptage_horaire.max())

        # Ajouter les données GeoJSON à la carte
        map = folium.GeoJson(data = districts_data, #géométrie des arrondissements 'districts'
                            name = "fréquentation moyenne par arrondissement",
                            style_function=lambda feature: {'fillColor': colormap(color_mean_dict[int(feature['id'])]) if  not pd.isna(color_mean_dict[int(feature['id'])]) else "black", # Appeler la fonction pour définir la couleur basée sur la moyenne
                                                            'fillOpacity': 0.8,
                                                            'weight': 0 , 'color' : 'black'     }, # Épaisseur de la bordure (0 pour aucune bordure)
                            highlight_function=lambda x: {'fillColor': '#ffffff'},  # Définir la couleur de surbrillance (blanc)
                            tooltip=folium.GeoJsonTooltip(fields=["District","Comptage_horaire"], aliases=["Arrondissement: ","Moyenne: "], localize=True)
                            ).add_to(fg0)


        # Ajouter les couches à chaque FeatureGroup
        ######## Couche reseauRevEtBois
        # Réseau Rev
        carte_REVe = df_reseau_lambert_carte[df_reseau_lambert_carte["reseau_REVe"]== 'Oui']
        folium.Choropleth(
            carte_REVe[carte_REVe.geometry.length>0.001],
            line_weight=6,
            line_color='blue').add_to(fg1)

        # Réseau Bois
        carte_bois = df_reseau_lambert_carte[df_reseau_lambert_carte["bois"]!= 'Hors bois']
        folium.Choropleth(carte_bois[carte_bois.geometry.length>0.001],
            line_weight=6,
            line_color='green').add_to(fg1)

        ######## Couche Typologie
        ########'Bandes cyclables', 'Autres itinéraires', 'Pistes cyclables','Couloirs de bus'

        typ_bande = df_reseau_lambert_carte[df_reseau_lambert_carte["typologie_simple"]== 'Bandes cyclables']
        folium.Choropleth(
            typ_bande[typ_bande.geometry.length>0.001],
            line_weight=6,
            line_color='orange').add_to(typ1)

        typ_autre = df_reseau_lambert_carte[df_reseau_lambert_carte["typologie_simple"]== 'Autres itinéraires']
        folium.Choropleth(
            typ_autre[typ_autre.geometry.length>0.001],
            line_weight=6,
            line_color='pink').add_to(typ2)


        typ_piste = df_reseau_lambert_carte[df_reseau_lambert_carte["typologie_simple"]== 'Pistes cyclables']
        folium.Choropleth(
            typ_piste[typ_piste.geometry.length>0.001],
            line_weight=6,
            line_color='purple').add_to(typ3)

        typ_bus = df_reseau_lambert_carte[df_reseau_lambert_carte["typologie_simple"]== 'Couloirs de bus']
        folium.Choropleth(
            typ_bus[typ_bus.geometry.length>0.001],
            line_weight=6,
            line_color='red').add_to(typ4)

        ######## Couche = CircleMarker
        ######## Couche = Marker
        # Regrouper et calculer la moyenne du comptage horaire par compteur
        df_reseau_map = df_pre.groupby(['Address', 'Latitude', 'Longitude', 'typologie_simple']).agg({'Comptage_horaire': 'mean'})

        # Réinitialiser l'index pour faire de 'Address', 'reseau_REVe', 'Latitude' et 'Longitude' des colonnes
        df_reseau_map.reset_index(inplace=True)

        # Calculer la moyenne des compteurs pour chaque bin
        df_reseau_map['Circle_color'] = pd.cut(df_reseau_map['Comptage_horaire'], bins = 5, labels = ['purple','blue','yellow','orange','red'])

        # Fonction pour définir la couleur des marqueurs
        def get_marker_color(type):
            if type =='Autres itinéraires' in df_reseau_map['typologie_simple'].values:
                return 'red'
            elif type =='Pistes cyclables' in df_reseau_map['typologie_simple'].values:
                return 'green'
            elif type =='Couloirs de bus' in df_reseau_map['typologie_simple'].values:
                return 'blue'
            else:
                return 'brown'

        df_reseau_map['Marker_Color'] = df_reseau_map['typologie_simple'].apply(get_marker_color)

        # Ajouter les cercles de compteur horaire moyen à la carte et les markers pour identifier les compteurs
        for index, data in df_reseau_map.iterrows():
            folium.CircleMarker([data['Latitude'], data['Longitude']],
                                radius=data['Comptage_horaire']/4,
                                tooltip=f'Compteur horaire moyen : {data["Comptage_horaire"]} <br> Nom du compteur : {data["Address"]}',
                                popup=f'Nom du compteur : {data["Address"]} <br> Moyenne : {round(data["Comptage_horaire"], 0)}',
                                color=None,fill=True, fill_color=data['Circle_color'], fill_opacity=0.6,
                                highlight=True).add_to(fg6)

            folium.Marker(location=[data['Latitude'], data['Longitude']],
                                tooltip=f'Nom du compteur : {data["Address"]} <br> Typologie : {data["typologie_simple"]}',
                                popup=f'Compteur horaire moyen : {data["Comptage_horaire"]} <br> Typologie : {data["typologie_simple"]}',
                                icon=folium.Icon(color=get_marker_color(data['typologie_simple'])),
                                highlight=True).add_to(fg2)

        ########## new 11/04
        ### Ajouter les markers des compteurs avec les 10 tops/ flops
        # Identifier les 10 meilleurs compteurs
        top10 = df_pre.groupby(['Address'], as_index=False).agg({'Comptage_horaire': 'mean'}).sort_values(['Comptage_horaire'], ascending=False).head(10)['Address']

        # Identifier les 10 compteurs les moins fréquentés
        flop10 = df_pre.groupby(['Address'], as_index=False).agg({'Comptage_horaire': 'mean'}).sort_values(['Comptage_horaire'], ascending=True).head(10)['Address']

        # Ajouter les 10 meilleurs compteurs à la carte avec une icône spécifique
        for compteur in top10:
            data = df_pre[df_pre["Address"] == compteur].iloc[0]
            folium.Marker([data["Latitude"], data["Longitude"]], icon=folium.Icon("orange"), tooltip=f'Nom du compteur : {data["Address"]} <br> "Compteur le plus déclenché"' ).add_to(fg7)

        # Ajouter les 10 compteurs les moins fréquentés à la carte avec une icône spécifique
        for compteur in flop10:
            data = df_pre[df_pre["Address"] == compteur].iloc[0]
            folium.Marker([data["Latitude"], data["Longitude"]],icon=folium.Icon("black"), tooltip=f'Nom du compteur : {data["Address"]} <br> "Compteur le moins déclenché" ').add_to(fg7)

        ##########

        ### Ajouter les FeatureGroups à la carte
        paris_map2.add_child(fg0)
        paris_map2.add_child(fg1)
        paris_map2.add_child(fg2)
        paris_map2.add_child(fg6)
        paris_map2.add_child(fg7)

        paris_map2.add_child(typ1)
        paris_map2.add_child(typ2)
        paris_map2.add_child(typ3)
        paris_map2.add_child(typ4)

        # Ajouter un Layer Control à la carte pour basculer entre la carte Choropleth et les marqueurs et cercles
        folium.LayerControl(collapsed=False, autoZIndex=True).add_to(paris_map2)

        # Faire les groupes
        GroupedLayerControl(
            groups={'Typologie': [typ1, typ2, typ3, typ4]},
            exclusive_groups=False,
            collapsed=False).add_to(paris_map2)


        # Afficher la carte dans Streamlit
        
        folium_static(paris_map2)

elif page == pages[6]:
    st.header("Conclusion")
    
    